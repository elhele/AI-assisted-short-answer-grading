%!TEX root = ../report.tex

\chapter{State of the Art}

\section{\textbf{Natural language processing}}

\subsection{Natural language processing development}

This work focuses on ASAG that is a very narrow subfield of Natural Language Processing (NLP). Therefore it is important to consider what actually is NLP first.\\

NLP is an very important research area that developed from the simple algorithms those could be performed on punch cards to huge sufisticated and heavy calculations those are performed by compamies like Google and Facebook now ~\cite{Cambria}. NLP is based on many other subject areas, such as linguistics, mathematics, computer science, artificial intelligence, robotics, and psychology. It can be applied in such areas as machine translation, summary generation, simplification of the texts, multilingual and cross-language information retrieval, diagnosing, chat-bots development, etc. ~\cite{Chowdhury}. The common idea among these tasks is that the computer must truely understand the language, the meaning of the phrases, sentences and whole texts. However, we cannot just tell the machine to "understand", its "understanding" must be formalized. This formalization is the subject of NLP. In a broader sense NLP task includes all levels of language processing and understanding: acoustic (speech recognition), analytical (syntactics and semantics), contextectual (anaphora, inference of referents, etc.), inferential (detection of inferences and implicatures), planning (sense, text and speech generation). Otherwise NLP can be narrowed mainly to analytical part, i.e. be considered as a set of techniques and methods for grammatical structure extraction and analysis for text understanding  ~\cite{Steedman}. Current work is not interested in such aspects of NLP as speech recognition and generation or in detailed research on grammars, the most interesting NLP aspect is an information extraction -- "subset of knowledge discovery and data mining that aims to extract useful bits of textual information from natural language texts" ~\cite{Chowdhury}.
However, text generation can be another useful tool for teachers who are interested in automated generation of examination questions, and speech recognition can be interesting for oral exams aytomation in the future. Therefore we will briefly consider all seven stages that may include a task of NLP ~\cite{Chowdhury}, ~\cite{Saad}:
\begin{itemize}
\item Phonetics
\item Morphology
\item Lexis 
\item Syntax
\item Semantics
\item Discourse
\item Pragmatics
\end{itemize}
 
Phonetics doesn't extract any meaning of the words, it is only focused on pronunciation. At this level an NLP system is focused more on a signal-processing than a linguistic part, though linguistic tools can be helpful in disambiguation of badly-parsed text.\\

Morphology is the most low-level stage of text processing: it considers the structure of the words, words' parts and how these parts influence the meaning of the words. For example, if "s" is added to a noun that would be a plurality morpheme and "s" added to a verb would indicate that it is the verb used in combination with a noun or pronoun that describes a third person.\\

Lexis considers the level of the words and parts of speech. Unlike in morphology at this level the meaning of the separate words is extracted regardless of their forms, e.g. the words "mouse", "mice" and "mouse's" can be the same lexical entity.\\

Syntax is the third level that considers the grammar of the language. It breaks the sentence into separate members of a sentence, i.e. subject (nominative), attribute (verb), case put after (object), etc., and reorganizes the data in such a way that can be analyzed by computer. This analysis includes grammar and parsing ~\cite{Saad}.\\

Semantic is a higher level analysis that considers extraction of meaning of words and sentences. \\

Discourse the highest level information extraction that considers the information extraction of text parts larger than a sentence, or the whole text. It deals with the whole structure of the document. The knowledge from this level can be very helpful for text summarization. \\

Pragmatics is the outer level of information extraction that includes the knowledge about the world, which is not contained in the document. It consists of two stages: finding ambiguities and disambiguation. The are following types of ambiguities: one-word long lexical (homonym) and referential ("it", "he", etc.), several-words long syntactic, sentence-long semantic, local (context requiring, which is usually a great issue for translation). Disambiguation includes the following methods: probabilistic (simple choice of the most common meaning of the word, optionally -- most possible in the subject field of this text), conditional probability (choice of the most common meaning of the word in the subject field of this text), context (most probable meaning of this word when it is surrounded by certain words or grammatical structure), world models (disambiguation by usage of world states)  ~\cite{Saad}. The conditional probability can be especially important for this work, because the tests are subject-specific and would normally use only one meaning of the certain word. For example, the word "operation" in a computer science exam will most likely not mean mean "surgery" . \\

History and prediction of integration of the methods mentioned above into NLP over is shown in Fig. \ref{fig:NLPLevels}. One can clearly see how the computers could grasp gradually more general meaning of the text. From understanding of single words to acquiring the meaning of whole sentences and paragraphs. Such differences are always easy to see in machine translation. One can compare the translation given by Google-translator (\url{translate.google.com}) that uses the bleeding-edge NLP-methods and PROMT (\url{http://www.online-translator.com/}) that translates almost word by word. PROMT uses rule-based and statistical machine translation, whereas Google-translator uses eight layers of Long Short-Term Memory-Recurrent Neural Network (LSTM-RNN). The original sentence was taken from the beginning of "Faust" by Johann Wolfgang Von Goethe: "Ihr n{\"a}hert sich wieder, schwankende Gestalten, Die vor sich selbst ist der Tr{\"u}ben Blick gezeigt.". The translation by Google is "You approach again, swaying figures, who in front of themselves is shown the murky gaze.", by PROMT -- "You approach again, fluctuating shapes, early to yourselves once the murky look shown.". PROMT couldn't even detect the German structure of the sentence and just put the English words in the initial order. Google translation is not perfect, but it is clear that it translates the whole sentence and not only the separate words. However, even a relatively good automated translator has problems with textual ambiguity and especially stylistics and it will take some time before it is able to translate the literature like human translators: "Again ye come, ye hovering Forms! I find ye, As early to my clouded sight ye shone!" ~\cite{Faust}.\\ 

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{img/NLPLevels}
    \caption{ NLP developement through three different eras or curves ~\cite{Cambria} }\label{fig:NLPLevels}
\end{figure}

Though semantic approach is slowly starting being used more commonly, syntactic approach is still used more often. That is why it is important to mention the main methods used there ~\cite{Cambria}:

\begin{itemize}
\item Keyword Spotting -- unambiguous buzzwords detection. It is used in the following systems:
\begin{itemize}
\item Ortony's Affective Lexicon -- for words categorization;
\item PageRank -- Google's page ranking algorithm;
\item TextRank -- graph-based ranking model for text processing.
\end{itemize}  
\item Lexical Affinity -- allows to detect not only unambiguous words, but also assign words to a certain group with a given probability based on a certain text corpora, e.g. "operation" can be medical with 40\%, computational with 30\% and military -- with 30\%. The problem of this approach is that it can easily be overfitted and work only for one area on which word probabilities it was trained.
\item Statistical NLP includes the following machine learning algorithms:
\begin{itemize}
\item maximum-likelihood
\item expectation maximization
\item conditional random fields
\item support vector machines (SVM)
\end{itemize}
The method allows to learn by training on a large text corpora the valence of the buzzwords, while also taking into consideration punctuation and how often certain words occur together. The problem of this method is semantic weakness: it's too focused on the buzzwords and cannot work efficiently enough with other words, therefore it requires a very large amount of text to train.
\end{itemize}

Semantic or concept-based level is fundamentally different: it is focused on the meaning of sentences as a whole and not on spotting of separate buzzwords, learning the concepts and not the words. One can distinguish the following concept-based NLP approaches ~\cite{Cambria}:
\begin{itemize}
\item internal knowledge-based techniques (endogenous) -- uses only information froinly to analytical part, i.e. be considered as a set of techniques and methods for grammatical structure extraction and analysis for text understam a given document. It integrates machine learning in order to conduct lexical-semantic analysis of a large amount of textual information to extract concepts. The following machine learning methods are used:  
\begin{itemize}
\item latent semantic analysis -- vector documents in a terms space;
\item latent Dirichlet allocation -- assigning terms to topics;
\item genetic algorithms -- probabilistic pseudo-stochastic search. 
\end{itemize}
\item external knowledge-based techniques (endogenous) -- used external information about the world. There are two main categories of those:
\begin{itemize}
\item taxonomic or ontology-based approaches are usually concerned about the "isA" relations between objects (e.g. "Albert Einstein") and concepts (e.g. "physicist"). The problems of this approach are bad scalability and that the representation of knowledge is too strictly-defined and typical. It overdefines the object and considers only its "proper" usage, for example an idea of a "key" can be related to a "picklock" or a "wireless keycard", as they are used to open a door, but not to other possible usages of the key, as a sort of weapon or a beer opener.
\item noetic or semantic knowledge-based include all human brain-based techniques. It focuses on semantic parsing nd linking separate linguistic components creating meaningful constructions. The following machine learning methods are used for this:
\begin{itemize}
\item neural networks
\item deep learning
\item sentic computing 
\end{itemize}
\end{itemize}
\end{itemize}



\subsection{From linguistics to NLP}


All the concepts mentioned above originate in pure linguistics and it is obvious that NLP area would strongly rely on it. However, it is a hard question, how much NLP should rely on pure linguistics and which modifications should be applied to linguistic methods so they would be efficiently applicable to NLP. Linguistics provides a highly-formalized model of the language. Raskin in ~\cite{Raskin} names the following manifestations and interpretations of linguistic formality:

\begin{itemize}
\item mathematical notation usage
\item mechanical-symbol-manipulaticn-device (MSMD) approach:
\begin{itemize}
\item collecting all information without any filtering and representing it in a mathematical form, e.g. formal grammar
\item creating a number of subsets of linguistic items with common features
\item all the subsets together must cover all the words those were considered
\item it is assumed that a native speaker must be able to differentiate all these items intuitively, without any specific knowledge
\end{itemize}
\end{itemize}

This approach seems formal enough to directly transfer it to a format acceptable for machine text processing without fundamental concept changes. However, it is not that easy and direct application of the method would usually lead to a bad result. One must know certain laws of a correct usage of pure linguistics in application to NLP. It is important to understand that the problem should originate in a target field (NLP), whereas approach and terms can come from the source field (linguistics). Otherwise it would be impossible to reach the target and use the source efficiently ~\cite{Raskin}. Plainly linguistic problems should not be formulated for NLP-tasks, however, linguistic approaches remain crucially important for NLP ~\cite{Steedman}. The NLP task includes the following ~\cite{Chowdhury}: 
\begin{itemize}
\item thought processes
\item representation and meaning of the language input
\item world knowledge of the program
\end{itemize}

Table \ref{NLPvsL} shows the main differences between NLP and pure linguistics those help to understand how to use linguistics for the task of NLP.\\

\begin{table}[]
\centering
\caption{Linguistics vs NLP ~\cite{Raskin}}
\label{NLPvsL}
\begin{tabular}{|l|l|}
\hline
 \makecell{Linguistics} & \makecell{NLP} \\ \hline
 \makecell{research on usage of sounds\\and its meaning for the language} & \makecell{ parsing spoken language to a written\\form for information extraction} \\ \hline
  \makecell{structuring the meaning of the text} &  \makecell{understanding the meaning of the text}  \\ \hline
  \makecell{categorizing different linguistic\\structure levels} &  \makecell{making a text source-independent by \\ extracting all relevant information}\\ \hline
 \makecell{separating linguistics and\\encyclopedic information} & \makecell{using encyclopedic information in \\combination with linguistic approaches} \\ \hline
 \makecell{formal (set of rules) representation\\of the result} & \makecell{making the information practically\\ accessible} \\ \hline
 \makecell{\textbf {language in general}} &  \makecell{\textbf{narrow part of the language}}  \\ \hline
\end{tabular}
\end{table}

The last point is especially important for the current work. It shows that it is common to use a very limited subset of the words existing in a language for NLP in general. It is also the case for our task, because each exam is dedicated to a certain subject with its own lexicon. Moreover, the formal nature of examinations allows to discard all informal and colloquial words. A student is normally not allowed to use sentences with slang and colloquial terminology similar to the following examples: "Yukio Mishima was a totally screwed wacko and a bent, but it is not the only reason why we all like him", "You should never forget to shiled the qubits properly, otherwise your experiment will be absolutely screwed". In the set of words those a formal exam would allow, "screw" as a verb should only mean "an act of turning a screw or other object having a thread" (\url{https://www.google.de/search?q=Dictionary}) and "bent" would only be a past or an adjective form of "bend".  That is why it is not efficient to try using general linguistic methods, though they are supported by the whole power of the language. Instead it makes sense to focus on NLP-methods those would allow to concentrate on specific subject-specific words. Raskin names the following advantages of the sublexicons ~\cite{Raskin}:

\begin{itemize}
\item on average it contains several hundred words instead of 500 000  
\item ambiguity is drastically reduced due to the reduced number of meaning homonyms (see examples above)
\item only a very limited extralinguistic knowledge of the world is needed
\end{itemize}

However, it is important not to forget that this approach can overlimit the lexicon and make the approach not extrapolatable enough. For example, if one develops an exam in the area of physics, he or she might want to reduce the set in such a way that a word "induction" would mean only "electromagnetic or magnetic induction" and it cannot mean "mathematical induction". It doesn't make much sense though, because adding this sense to such word is not very costly and in general it is possible to use a similar set of words for mathematics and physics, which would allow to have more train exam samples. Moreover, this approach is dangerous because even in one area it is not always possible to foresee all possible vocabulary that will be used.

\subsection{NLP methods for ASAG}

In this section some general NLP methods those are used in various ASAG systems are considered. More specific, e.g. syntactic pattern matching or parse tree matching, are considered in details in ASAG section.\\

\textbf{N-grams} is a method that breaks a sentence into a set of word sequences of certain length. For example, bigrams and trigrams for a sentence "Penrose's model of quantum consciousness is rather questionable" are the following: bigrms -- \{"Penrose's model", "model of", "of quantum", "quantum consciousness", "consciousness is", "is rather", "rather questionable"\}, trigrams -- \{"Penrose's model of", "model of quantum", "of quantum consciousness", "quantum consciousness is", "consciousness is rather", "is rather questionable"\}. A similar approach can be used on a word level. For example, bigrams for a word "quantum" are the following: \{"qu", "ua", "an", "nt", "tu", "um"\}.\\

\textbf{Skip-grams} is a method in which "n-grams are still stored to model language, but they allow for tokens to be skipped" ~\cite{Skip-gram}. Guthrie  ~\cite{Skip-gram} defines k-skip-n-grams for a sentence containing words $w_1,w_2,...,w_n$ in the following way:

\begin{equation} \label{eq:kskip}
\{w_{i1},w_{i2},...,w_{in} | \sum_{j-1}^n i_j - i_{j-1} < k\}, 
\end{equation}

where n is strictly higher than one. For better understanding of the concept the following example can be made. A sentence "A new art colour for our Irish poets: snotgreen. " ~\cite{Ulysses} broken into bigrams and 2-skip-bigrams takes the following form: bigrams = \{"A new", "new art", "colour for", "for our", "our Irish", "Irish poets", "poets snotgreen"\}, 2-skip-bigrams = \{"A new", "A art", "A colour", "new art", "new colour", "new for", "for our", "for Irish", "for poets", "our Irish", "our poets", "our snotgreen", "Irish poets", "Irish snotgreen",  "poets snotgreen"\}. One can see that in many cases it creates senseless sequences of the words like "A art", however, k-skip-n-gram "for Irish" would make total sense, because the sentence without word "our" is as valid as the initial one. Moreover, the wrong k-skip-n-grams are too random and often grammatically senseless, that is why they can with very low probability create false positives, which means that it is at least harmless to use them.\\

\textbf{Tokenization} is breaking a sentence into separate words – tokens – and removing punctuation, which is basically extraction of unigrams ~\cite{token}. For example, a sentence "Tokenization is breaking a sentence into separate words – tokens – and removing punctuation, which is basically extraction of unigrams" will take the following form: \{Tokenization"", "is","breaking", "a", "sentence", "into", "separate", "words", "tokens", "and", "", "removing", "punctuation", "which","unigrams","is","basically","extraction","of","unigrams"\}. \\

\textbf{Stemming and lemmatization} are techniques those allow to returned a word in a specific form to its original form, e.g. a verb "is" is lemmatized as "be", genitive case "des Herzens" -- as "das Herz", plural "mice" -- as "mouse", etc. ~\cite{lemm}.\\

\textbf{Spelling correction} allows to change the wrongly written words to the correct form, e.g. "corrrect" -- "correct". However, it can be dangerous in cases when there are several possible correction options: "fhy" -- "fry" or "fly".\\

\textbf{Noun phrase chunking} is a method for breaking sentences into subconstituents, i.e. verbs, nouns, and prepositional phrases ~\cite{chunk}. For example: "The saddest music in the world." -- \{("the", "DT"), ("saddest", "JJ"), ("music", "NN"), ("in", "IN"), ("the", "DT"), ("world", "NN") \}, where "DT" is a definite article, "JJ" -- adjective, "NN" -- noun and "IN" -- preposition.\\

\textbf{Pronoun or anaphora resolution} -- methods those allow to identify to which word refers a pronoun or anoun phrase. For example, in a sentence "One of the outstanding traits of Nabokov's writing is that he could write, and actually wrote, equally well in Russian and English, which allows speakers of both languages read his books in original." "he" is "Nabokov" ~\cite{anaphora}.\\

\textbf{Stop words spotting and removal} -- finding and removing overly common and thereby semantically non-selective words, such as "a", "an", "the", "its", etc. ~\cite{stop}. For example "A cat sit on a mat." transforms to "cat sit mat".\\

\textbf{Part Of Speech (POS) tagging} -- assigning part of speech to each word in a sentence: "Many ideas of Buddhismus are strongly supported by evolutionary biology." -- \{ ("many", "adjective"), ("ideas", "noun"), ("of", "preposition"), ("Buddhismus", "noun"), ("are", "verb"), ("strongly", "adjective"), ("supported", "verb"), ("by", "preposition"), ("evolutionary", "adjective"),  ("biology", "noun")\} ~\cite{POS}.


\section{\textbf{Automated grading}}

Research in natural language processing (NLP) started in 1950. The following topics were and still are the main focus of NLP researchers: automatic translation, information retrieval, summarizing of texts, question answering, information extraction and topic modeling  ~\cite{Cambria}. Sixteen years later, in 1966, the first work on automated grading was published ~\cite{Page}. That first work was dedicated to essay grading and it laid a foundation to a large field that now includes automated essay and short answer grading. The main differences are listed in Table \ref{ASAGvsAES} ~\cite{Burrows}, ~\cite{Hasanah}, ~\cite{Ziai}. This section is focused on analysis of current state of the art in these two areas.

\begin{table}[h!]
\centering
\caption{ASAG vs AES}
\label{ASAGvsAES}
\begin{tabular}{|l|l|l|}
\hline
 & Short answer &  Essay \\ \hline
 Length & $\leq$ one paragraph & $>$ one paragraph  \\ \hline
 Evaluation criterion & Content &  Style, content \\ \hline
 Openness & Closed &  Open \\ \hline
 Question format & Formulated according to strict rules & Can be general \\ \hline
 Correct answer & Exists & Doesn't exist \\ \hline
\end{tabular}
\end{table}



\subsection{AES}

\subsubsection{Overview of the systems}

In recent years massive tests, such as language proficiency exams (TOEFL, IELTS, TestDaF, DELF, etc.) or MOOC assessments, are becoming increasingly popular. All language tests and many MOOC assignments contain parts where the students have to either summarize a given text or write his/her own text or essay. Scoring these long text is a huge task that requires tremendous amount of human power. Language tests are rather expensive, thereby it is possible to find enough people to grade them. The idea of MOOCs, however, is to make the courses from the best universities affordable for people and that is why it is why scoring is a big issue in this case. It is possible to automate multiple-choice tests, but the quality of such check of knowledge is very low. Therefore, it was essential to automate free text grading as well. Moreover, it is still important for the language tests, because the autograding would allow to give a faster and less biased response.\\

Both, automated and human essay grading, have several important issues those must be faced -- see table \ref{Issues} ~\cite{Blood}. Therefore, starting the automated grading was a long process, but now it is rapidly developing. \\


\begin{table}[]
\centering
\caption{Issues of machine and human grading}
\label{Issues}
\begin{tabular}{|l|l|}
\hline
 \makecell{Automated grading} & \makecell{Human grading} \\ \hline
 \makecell{It is hard to define an impersonal criterion\\ for text style quality: many long-text\\ tasks require not only correctness of content,\\ but also a good writing style and/or usage\\ of certain constructions. In tasks\\ like "Describe a graph", which a participant has\\ to do on TestDaF and IELTS, one should only\\ use certain constructions and be relevant, however,\\ in essay part the style, which\\ can be estimated differently even by\\ human examiners, plays an important role. } & \makecell{Human graders must be\\ specifically trained, which requires\\ a massive amount of time and money.} \\ \hline
 \makecell{Though it is not intentionally malicious,\\ autograder can still be biased. For example,\\ if students with similar writing style \\usually deliver works of bad quality,\\ the system can overlearn in such a way that\\ it would rate badly even good works of\\ similar style.} & \makecell{ Human graders are biased more likely\\than a machine. Therefore\\test organizations try to avoid\\such bias by a complex\\ cross-validation, testing and\\ changing the assignments in such a\\way that the possible bias is\\ reduced, which requires a\\lot of time and\\financial resources. } \\ \hline
  \makecell{Usage of AES may cause negative reaction of \\examenees.} &  \makecell{The high costs reduce the number of\\people who can afford to take an exam.}  \\ \hline
    \makecell{AES might have problems with gibberish\\detection.} &  \makecell{"Human factor" like sickness\\ or any other problems\\of the grader may cause\\a significant increase of grading\\time or decrease of grading\\ quality. }  \\ \hline
\end{tabular}
\end{table}


The first multiple-choice grader was developed in 1938, whereas the first idea of the free text grader appeared only in 1960's in a work of Ellis Page ~\cite{Blood}. The system was called Project Essay Grade (\textbf{PEG}). The approach was based on usage of proxes and trins -- the variables recognizable by a machine and a human graders accordingly, e.g. a number of words could be a prox and elaboration -- a trin. The method was based on training the multiple-regression algorithm to grade like humans in a train set, not on the analysis of the text itself. The main features of the texts were the number of words and word length, which allowed to reach a correlation of 71\% (77\% in the later releases) with the human graders. The correlation between the machine grading and human graders was shown to be higher than the correlation between different human garders ~\cite{Blood}. This grader, however, was very vulnarable to "gibberish" texts, i.e. the ones containing completely senseless words like "bzhykhlvyazsk" or "bdvfngjvsbcdhkuhlhdsskmlklewnmx" ~\cite{Shermis}. The words are long and the text can be as long as the writer please, however, it is not a real essay and by a human teacher it would be graded with zero points. The 77\% correlation was obtained in absence of such important test cases. It shows a great importance of proper dataset that would cover most of the possible test cases. \\

Almost 40 years after the development of Page's approach of using the human graders' scores instead of a real understanding of text by computer as a main training criterion was of current interest. For example, \textbf{E-Rater} developed by ETS (the organization that conducts TOEFL and GRE) in 1990's operated in the similar way and also used multiple-regression method combined, however, with NLP, which allowed to include grammatical accuracy. The latest (2006) version had an additional feature that took into account word overlap between test and train essays, which was considered rather advanced, because it allowed to reduce importance of essay length, which essentially should not be an indicator of quality. This system is now used in GRE, TOEFL and several other ETS's tests. Correlation with human scores is very high -- 97\%, again higher than between human raters ~\cite{Blood}. However, there is still a lot of criticism. For example, it was shown that examnee can submit a text -- so-called "word salad" -- that is long enough, contains all right and long words and constructions, but still doesn't make any sense, and the system will still grade it highly, for it meets all the needed requirements ~\cite{Hearst}. Another point voiced by critics is that the system can grade a good and creative essay, which doesn't meet some formal requirements, badly. That is another reason why many people including prominent scientists like Noam Chomsky argue that AES brings no good to the education ~\cite{petition}. It is true, however, such requirements is one of the examples when people expect from machine a way better performance than from humans. A human rater receives a massive amount of written works those must be graded in a short time. Human grading in this case is as formal as machine grading and creativity can be punished by a bad grade as possibly as in case of machine grading. Moreover, human graders are shown to have less unity among each other in their judgements than with machine, as it was shown in experiments with both, PEG and E-Rater ~\cite{Blood}. Individual approach is important, but it is feasible only in small groups. In case of massive courses it is equally formal, no matter if it is an automated or human grading.\\

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{img/IEAArch}
    \caption{Architecture of IEA ~\cite{Dikli}.\label{fig:IEAArch}}
\end{figure}

PEG and E-Rater had the same base idea. A fundamentally new approach was introduced in 1998 by Landauer in "An Introduction to Latent Semantic Analysis" (LSA) ~\cite{LSA}. The main difference was that the method started caring about the meaning of the text and not only on breaking it into linguistically non-meaningful features those will be weighted and learned in relation to the human grades. In this method statistical  computations on a large corpus of text are used to extract the contextual meaning of words, which places the method semantic part NLP curve (Figure \ref{fig:NLPLevels}) ~\cite{LSA}. Moreover, the goal of methods developers was to imitate the way humans obtain and classify knowledge from the piece of text, as well as they resolve ambiguities by context ~\cite{Blood}. LSA-based essay grader was called Intelligent Essay Assessor (\textbf{IEA}). Its architecture is shown in Fig. \ref{fig:IEAArch}. This system is closer to ASAG than classical AES, because it focuses only on context and not the writing style. Moreover, it doesn't use the human raters' grades as a reference value, but learns based on essay examples -- domain-representative text. These essays are divided in quality categories, and a text to be graded is compared to them and rated with the same grade that the most similar essays. The main idea of LSA is an collecting of various contexts of a word and thereby word meaning similarity and possible text patterns where the word may occur. LSA can be used for estimation of word-word, word-phrase and phrase-phrase similarity. It is important that for this LSA doesn't use any external knowledge. The method consists of the following steps:

\begin{itemize}
\item a text must be represented as a matrix, in which each row is a unique word and each column -- context of the corresponding word. For example, for a set of sentences \{("The sun is bright.", "S1"), ("Such a bright, bright student!", "S2"), ("Lying student is lying in the sun.", "S3"), ("That's a lie!", "S4")\} it may take the form shown in Table. \ref{LSA} (note: the sentences went through lemmatization and stop words removal, because only the meaningful words are of interest). This example also shows how important it is to have a large text corpora in order to use LSA efficiently: in these four sentences words "bright" and "lie" were used in completely different meanings and thereby it is not possible to derive much sense from them.
\begin{table}[h!]
\centering
\caption{LSA matrix example}
\label{LSA}
\begin{tabular}{|l|l|l|l|l|}
\hline
        & S1 & S2 & S3 & S4 \\ \hline
sun     & 1 & 0 & 1 & 0 \\ \hline
bright  & 1 & 1 & 0 & 0 \\ \hline
student & 0 & 1 & 0 & 0 \\ \hline
lie     & 0 & 0 & 2 & 1 \\ \hline
\end{tabular}
\end{table}

\item Single Value Decomposition (SVD) is applied to the matrix. In SVD a rectangular matrix $\boldsymbol{M}$ of a size $m \times n$ is decomposed in the following way:
\begin{equation} \label{eq:SVD}
\boldsymbol{M} = \boldsymbol{U \Sigma V^*}, 
\end{equation}
where $\boldsymbol U$ is a unitary matrix of a size $m \times m$, $\boldsymbol{\Sigma}$ -- diagonal matrix that contains non-negative real numbers of a size $m \times n$ and $\boldsymbol{V^*}$ -- conjugate transpose of a unitary matrix  of a size $m \times n$. SVD-decomposition for the matrix above is shown below:

\begin{equation} \label{eq:U}
\boldsymbol U = \begin{pmatrix}
sun & -0.46 & 0.65 & 0.67 & 0.51 \\
bright & -0.12 & 0.81 & 0 & -0.57 \\
student & -0.02 & 0.43 & -0.67 & 0.61 \\
lie & -0.88 & -0.28 & -0.33 & -0.21
\end{pmatrix}
\end{equation}

\begin{equation} \label{eq:Sigma}
\boldsymbol {\Sigma} = \begin{pmatrix}
2.46 & 0 & 0 & 0 \\
0 & 1.7 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0.24
\end{pmatrix}
\end{equation}

\begin{equation} \label{eq:V}
\boldsymbol V* = \begin{pmatrix}
S1 & S2 & S3 & S4 \\
-0.24 & -0.06 & -0.09 & -0.36 \\
 0.65 & 0.73 & -0.15 & -0.16 \\
 0.67 & -0.67 & 0 & -0.33 \\
-0.28 & 0.15 & 0.4 & -0.86 \\
\end{pmatrix}
\end{equation}

\item Next step is dimensionality reduction: one can removes coefficients from  $\boldsymbol{\Sigma}$ and keeps only as much coefficients as much dimensions are needed. In the case, for example, we can take two dimensions, so 1 and 0.24 can be nullified. In this case initial matrix takes the form shown in Table \ref{LSAR}. After nullification the SVD equations will take the following form:

\begin{equation} \label{eq:U0}
\boldsymbol U = \begin{pmatrix}
sun & -0.46 & 0.65 \\
bright & -0.12 & 0.81 \\
student & -0.02 & 0.43 \\
lie & -0.88 & -0.28 
\end{pmatrix}
\end{equation}

\begin{equation} \label{eq:Sigma0}
\boldsymbol {\Sigma} = \begin{pmatrix}
2.46 & 0 & 0 & 0 \\
0 & 1.7 & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0
\end{pmatrix}
\end{equation}

\begin{equation} \label{eq:V0}
\boldsymbol V* = \begin{pmatrix}
S1 & S2 & S3 & S4 \\
-0.24 & -0.06 & -0.09 & -0.36 \\
 0.65 & 0.73 & -0.15 & -0.16 \\
\end{pmatrix}
\end{equation}

\begin{table}[h!]
\centering
\caption{LSA reduced dimensionality example}
\label{LSAR}
\begin{tabular}{|l|l|l|l|l|}
\hline
 & sun & bright & student  & lie \\ \hline
S1 & 0.59 & 0.43 & 0.04 & 0.33 \\ \hline
S2 & 0.96 & 1.02 & 0.95 & 0.12 \\ \hline
S3 & 0.49 & 0.53 & 0.06 & -0.1 \\ \hline
S3 & 0.21 & -0.22 & 2.02 & 0.85 \\ \hline
\end{tabular}
\end{table}

\item Based on the decomposition the words are clusterized by sense over the chosen number of dimensions: words coordinates are taken from $\boldsymbol U$ and sentences -- from $\boldsymbol{V^*}$. In Figure \ref{fig:LSAExample} one can see the results of LSA clusterized by k-Nearest-Neighbor (kNN) for the example we consider. Because of the size of the "dataset" the results are not satisfactory, but one can already see that the word "lie" right near "S3", where it occurred twice and relatively close to "S4" though it was not put into the same cluster. Word "bright" is in one cluster with both sentences it was in. "Student" is in one cluster with "S2" and not far from "S3", and "sun" is clusterized incorrectly, but it is between "S1" and "S3", so it could be classified better if more dimensions were used. Implementation of this example can be found via the link: \url{https://github.com/elhele/AI-assisted-short-answer-grading/blob/master/src/LSA_tiny_example.py}.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{img/LSAExample}
    \caption{LSA clustering for the following set of sentences: set of sentences \{("The sun is bright.", "S1"), ("Such a bright, bright student!", "S2"), ("Lying student is lying in the sun.", "S3"), ("That's a lie!", "S4")\}.\label{fig:LSAExample}}
\end{figure}

\end{itemize}

This method can fail in case an essay is too original, but in this case the text can be just marked as "unclassifiable" and be sent to the human teacher. The method was tested with Graduate Management Admission Test$^{\tiny{\textregistered}}$ (GMAT$^{\tiny{\textregistered}}$) essays and showed correlation with human raters of 86\%, whereas the correlation among human raters was 87\% and 86\% for different essay types. It is interesting that IEA was very positively met by students -- 98\% of the ones who tested it very willing to take more tests graded by this system. The system has a user-friendly interface -- see Fig. \ref{fig:IEAInt}. \\

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{img/IEAInt}
    \caption{Architecture of IEA ~\cite{Dikli}.\label{fig:IEAInt}}
\end{figure}

Next important step in AES was an integration of artificial intelligence. \textbf{IntelliMetric$^{TM}$} or its enterprise version MY Access!$^{\tiny{\textregistered}}$ didn't follow coprus-based path of LSA work and, as PEG e-rater, mostly relies on NLP. However, it tries to emulate the work of human brain and uses CogniSearch$^{TM}$ -- an NLP AI system, which is able to perform POS tagging, morphological analysis, spelling recognition, and collocation grammar (the whole algorithm is, however, not published openly and  protected by patents). It allows the system to go deeper into the language structure and rate essays in accordance to the norms of language -- see Figure \ref{fig:IntelliMetric}. For grading the systems requires a large set of pre-graded essays. The system was tested with GMAT$^{\tiny{\textregistered}}$ prompts and showed human-computer correlation of 97\% on average. MY Access!$^{\tiny{\textregistered}}$ is widely used in USA: in California, Pennsylvania, Massachusetts, Virginia, Oregon, and Texas. Moreover, the following companies use the system as well: The College Board, ACT, Harcourt Assessment, Inc., CTB/McGraw Hill, Thomson Learning. Microsoft, Apple Computer, AOL and Sun Microsystems. 

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.4\textwidth]{img/IntelliMetric}
    \caption{IntelliMetric$^{TM}$ grading model ~\cite{Dikli}.\label{fig:IntelliMetric}}
\end{figure}

Bayesian Essay Test Scoring sYstem$^{TM}$ (\textbf{BETSY}) ~\cite{BETSY} is a probabilistic AES system that performs text classification based on a large training corpus (1000 texts). To rate the essays BETSY performs the following:

\begin{itemize}
\item words and word pairs training
\item database statistics evaluation
\item uncommon words and word pairs removal
\item stop words removal
\item lemmatization
\item removal of wrongly classified essays 
\end{itemize}

The system uses such NLP features as word frequency in a certain context, specific words and phrases, sentence length, number of verbs, noun-verbs pairs. It combines PEG, early e-rater and LSA methods.\\

One of the important steps in AES was a kaggle competition \textbf{"The Hewlett Foundation: Automated Essay Scoring"} ~\cite{HewlettKaggle}. The winning algorithms are not published, but there is an open information about some principles of their work from a winner of a second place. His team uses the following features: number of sentences, lengths of sentences and words, and presence or absence of certain words. The features were combined with random forest (RF) and regression. An important feature of the approach is that the system can detect a work, which cannot be graded with a good confidence, send it to a human rater and include the grade he/her gave to this work into the train dataset. This method has a name "active learning" ~\cite{vikGit}. The dataset contains eight subsets with 12978 essays in a test set and 4254 in a train set -- see "Essay\_Set\_Descriptions.zip" that can be downloaded on the following page: \url{https://www.kaggle.com/c/asap-aes/data}, last visited 20.04.2018, registration is required. Essays length is between 150 and 550 words ~\cite{Madala}. \\

The main idea of \textbf{Cummins}'s~\cite{Cummins} work is a generalization of AES: he claims that the most hindering problem of most of the AES systems is that they are not general enough and require a lot of retraining each time a new essay badge has to be graded. Cummins addresses this problem by using multi-task learning, which grasps the general and specific features of the essays by using a shared-representation vectorized model of the essays, and preference-ranking model, which is based basically on a perceptron classifier. All essays are tokenized. The following features are used:
\begin{itemize}
\item POS tagging
\item n-grams (1-3)
\item unique words number
\item words length
\item sentences length
\item linking words presence ("nevertheless", "moreover", etc.)
\item grammatical and spelling mistakes
\item grammatical relations between constituents in a clause
\end{itemize}
A dataset from "The Hewlett Foundation: Automated Essay Scoring"  was used to evaluate the algorithm. Human-computer correlation is between 60\% and 80\% for different essay subsets.\\


The most well-known and widely used approaches has already been named, however, it is important to consider at least one of the newest approaches. One of the latest AES systems was developed by \textbf{Madala} ~\cite{Madala} in 2018. Nine NLP features are used for the grading: 
\begin{itemize}
\item reachness of the used vocabulary 
\item relation between a given word number limit and actual length of an essay
\item length of sentences
\item voice -- active is preferred
\item tense -- must be consistent
\item spelling
\item grammar
\item semantic similarity:
\begin{itemize}
\item between the essay and a given topic
\item between the parts of essay -- how coherent it is
\end{itemize} 
\end{itemize}

The features are preprocessed and combined with the following machine learning techniques: kNN, linear regression and SVM. The system is implemented in Python and uses the following libraries: NLTK, SpaCy. A dataset from "The Hewlett Foundation: Automated Essay Scoring"  was used to evaluate the algorithm. The results, depending on the essay type, are in range between 73\% to 93\%. \\

Table \ref{AES} summarized the AES systems' performance over the years.\\

\begin{table}[h!]
\centering
\caption{Comparison of AES systems}
\label{AES}
\begin{tabular}{|l|l|l|l|}
\hline
\makecell{Year} & \makecell{System} & \makecell{Approach}  & \makecell{Accuracy} \\ \hline
\makecell{1968} & \makecell{PEG} & \makecell{comparison to human grading,\\ multiple-regression}  & \makecell{77\%} \\ \hline
\makecell{1999} & \makecell{IEA} & \makecell{domain-representative text, \\ LSA}  & \makecell{86\%} \\ \hline
\makecell{2003} & \makecell{IntelliMetric$^{TM}$} & \makecell{comparison to human grading, \\ NLP + unknown \\ patented techniques}  & \makecell{ 97\%} \\ \hline
\makecell{2005} & \makecell{BETSY$^{TM}$} & \makecell{large essays corpus,\\ NLP-features,\\ Bayesian theorem}  & \makecell{ 80\%} \\ \hline
\makecell{2012} & \makecell{The Hewlett Foundation:\\ Automated Essay Scoring} & \makecell{comparison to human grading,\\ NLP, \\ RF and \\ linear regression}  & \makecell{ 81\%} \\ \hline
\makecell{2013} & \makecell{E-Rater} & \makecell{comparison to human grading, \\ multiple-regression\\ + NLP}  & \makecell{97\%} \\ \hline
\makecell{2016} & \makecell{Cummins} & \makecell{comparison to human grading,\\NLP-features,\\multi-task learning,\\preference-ranking model\\on a perceptron }  & \makecell{ 60\%-80\%} \\ \hline
\makecell{2018} & \makecell{Madala} & \makecell{comparison to human grading,\\NLP features, kNN,\\ linear regression,\\ SVM}  & \makecell{ 73\% to 93\%} \\ \hline
\end{tabular}
\end{table}


\subsubsection{Gibberish detection}

The systems considered in a previous section show impressive results. However, even the most recent ones are vulnerable to various sorts of gibberish, because they mostly use the datasets with real tests written by the students who are were unaware of being graded automatically. Therefore, it is important to consider the topic of gibberish detection separately. Moreover, some of the gibberish detection techniques can be used for ASAG as well, though most of the ASAG systems don't put enough attention to this problem.\\

There are the following types of gibberish ~\cite{gibberishShermis}, ~\cite{gibberish} those also obey the NLP development curve from Figure \ref{fig:NLPLevels}:
\begin{itemize}
\item word level ("keyboard-banging") -- words don't make sense and represent random letters sequences, e.g. "bzhyhlvyazsk". PEG was vulnerable even to this gibberish type. This can be spotted by detecting ill-formed words.
\item sentence level ("word salad") -- essay contains real words, but in a random order, e.g. "Went umbrella grump. Handbook keyboard trump. Begone.". The previous and this gibberish types were solved in e-rater. It can be spotted by a sentence analysis and POS tagging.
\item paragraph level ("copied-prompt") -- essay only or mostly consists of an essay topic copied multiple times. This gibberish type can be easily detected by spotting the phrases from the topic in the text.
\item text level:
\begin{itemize}
\item "unexpected-topic" -- essay is a sensible well-written text, but on a topic completely different from the required one, e.g. the topic was "Character of Natasha Rostova from War and Peace" and the essay was about mating habits of baboons.
\item "bad faith" -- each part of essay separately makes sense and related to the main topic, but the parts are incoherent to each other.
\end{itemize}
For these gibberish types Madala offers a semantic similarity solution, the essay should be similar to the topic and its parts -- to each other.
\end{itemize}

ASAG systems are unlikely to be vulnerable to "keyword-banging" and "unexpected topic", because in one way or another they normally use keyword spotting. "Copied-prompt" issues are also unlikely, because the question for ASAG must not contain too much information about the topic. However, ASAG can have problems with a word salad in case in contains important words from the example answer. "Bad faith" answer can also be the case, because the sentences in an answer can be put in a wrong, non-sensible order.


\subsection{ASAG}

ASAG is a form of automated grading of texts that focuses on short answers -- from one sentence to one paragraph. Apart from that the answer must contain some external knowledge not mentioned in the question and the grader mostly concerns about the meaning of the answer and not about writing style, though the last point is under discussion, because a professor or teacher might want to include such grading criteria as spelling or grammar. ASAG has the same benefits in comparison to human graders those are mentioned in Table \ref{Issues}, but it has clearer criteria of correctness and it is less vulnerable to gibberish, because the ASAG solutions usually use more concrete criteria than AES. For example, if the simplest AES system was based only on text and word lengths, the simplest ASAG system would care about keywords and thereby it is less vulnerable to gibberish. Therefore, a simplified ASAG task is more feasible than simplified AES. "Simplified" is because in the end both systems from different starting points are heading  to the truly-human understanding of meaning and writing style, thereby becoming rather similar.\\

However, while the ASAG systems are still limited, it is important that answers and also questions should be formalized. The questions must meet certain criteria ~\cite{Hasanah}:
\begin{itemize}
\item The question must not contain any information that can let a student to obtain the answer from the question, i.e. the student must be able to formulate the answer based only on his or her external knowledge;
\item In general the question must require a strictly natural language answer. Current work focuses on the natural language part. However, in our project we are going to allow a combination of natural language, formulas, pictures and programming code.
\item The question must be formulated in such a way that student could give an answer between one sentence and one paragraph length.
\item The question must be formulated in the least ambiguous way to increase the level of close-ended responses.
\end{itemize}

When the correctly-formulated questions are prepared and the student answers obtained, one can start building an ASAG system. A general approach for development of such a system is shown in Fig.\ref{fig:workflowEN}. A historical overview of different approaches to ASAG-systems is shown Gantt chart below ~\cite{Hasanah}, ~\cite{Burrows}. The following part of the current work is dedicated to detailed consideration of these approaches.\\

\begin{figure}[h!]
  \centering
  \includegraphics[width=1.0\textwidth]{img/workflowEN}
    \caption{A general workflow for an ASAG system developement ~\cite{Burrows}.\label{fig:workflowEN}}
\end{figure}

\begin{preview}
\centering
\tikzset{every picture/.style={xscale=0.7,transform shape}}
  \begin{gantt}{6}{10}
    \begin{ganttitle}
    \titleelement{1995-2000}{2}
    \titleelement{2000-2005}{2}
    \titleelement{2005-2010}{2}
    \titleelement{2010-2015}{2}
    \titleelement{2015-2018}{2}  
    \end{ganttitle}
    \ganttbar{\textbf{Concept mapping}}{0}{7.5}
    \ganttbar{\textbf{Information extraction}}{2.5}{4}
    \ganttbar{\textbf{Corpus-based methods}}{2.9}{3.2}
    \ganttbar{\textbf{Machine learning}}{5.4}{4.6}
    \ganttbar{Deep learning}{9}{1}
  \end{gantt}
\end{preview}

\subsubsection{Concept/facet mapping}

The oldest direction in ASAG research is, in its pure form, mostly interesting from the historical point of view, for it is not being actively used nowadays. The main idea is searching for the presence/absence of certain concepts those students can mention on their answers. This can be used either on a sentence or facet level. The facet normally contains three or two words ~\cite{Burrows}. This approach is in a way similar to textual entailment, because entailment also concerns, if one sentence means the same that another, contains the same concept -- entails it -- or not.\\

The biggest disadvantage of this approach for ASAG is that it can detect the presence of the concept, but not if it was mentioned in a correct way ~\cite{Dzikovska}. For example, a student can write "Q: What is PCA? A: PCA is a method that affects dimensionality. It increases it.", which is incorrect, but the algorithm may still accept or partially accept this answer, because PCA really affects the dimensionality and it might match the concept, whereas the second part can be not accounted into the same concept estimation.\\

The first concept mapping system was developed by \textbf{Burstein} in 1996 ~\cite{Burrows}. It uses  Lexical Conceptual Structure representation, which means that the system detects a presence of certain words and grammatical structures. The approach was rather limited, because it only allowed a certain type of question -- the  hypothesis-style, which means that the answer to this question must contain several explanations of the hypothesis.\\

Another mapping system is Automatic Text Marker (\textbf{ATM}). It operates with facets: ATM divides example and students' answers into pieces containing a couple of words each, and estimates the number of common concepts in those small parts of example ans students' answers. It is important that a higher weight can be assigned to the piece with a keyword ~\cite{Burrows}.\\

A concept mapping system developed by ETS is called a Concept Rater or \textbf{c-rater}. It focuses on sentence-level comparison of example and students' answers. An example answer is intentionally formulated in such a way that each sentence contains only one concept. Sentence comparison is rule-based and includes a paraphrase recognizer that can distinguish syntactic variations, pronoun references, morphological variations, and synonyms. Moreover, it uses spelling correction ~\cite{c-rater}. \\

Later concept mapping stated including machine learning. \textbf{Wang} in ~\cite{Wang} compares pure heuristics-based concept mapping with  data-driven classification by support vector machines (SVM) and combination of those two methods. In a combination of methods SVM was used with bag-of-words features, whereas pure machine learning used unigrams and bigrams. The best result was reached using the combination of methods.\\

Table \ref{CFM} shows the comparison of methods above. Accuracy between certain numbers means that the algorithm gives different correlation with human graders on different datasets.

\begin{table}[h!]
\centering
\caption{Concept/facet mapping systems}
\label{CFM}
\begin{tabular}{|l|l|l|l|}
\hline
 System & Year & Approach & Accuracy \\ \hline
 Burstein & 1996 & lexical conceptual structure representation & 48\% to 83\% \\ \hline
 ATM & 2001 & weighted comparison of few-words facets & unknown \\ \hline
 c-rater & 2003 & sentence-level paraphrase recognition & 87\% to 94\% \\ \hline
 Wang & 2008 & concept mapping with SVMs & 92\% \\ \hline
\end{tabular}
\end{table}

\subsubsection{Information extraction}

Information extraction (IE) has already been cosidered in NLP section of the work. For ASAG it is also basically a pattern or fact template matching, template merging and resolution of referential ambiguities ~\cite{Burrows}, ~\cite{Pulman}. It allows to extract a structures information from unstructured sources. A significant advantage of this approach is its robustness. To certain extend it is immune to grammatically wrong sentences and therefore requires less text preprocessing. The main IE approches are listed below ~\cite{Hasanah}, ~\cite{Pulman}: 

\begin{itemize}
\item \textbf{parse tree matching} uses specific templates for correct and wrong answers. Each answer is being preprocessed and compared to templates with different grades. An example of such approach is \textbf{AutoMark} grader.
\item \textbf{regular expression matching} applies a regular expression patter to all elements of the students' answers. Examples of such approach are \textbf{WebLAS} and \textbf{PMatch} graders:
\begin{itemize}
\item  WebLAS is an older system. It automatically extracts the patterns from example answers and then offers the grader to approve it and its synonyms and give a certain weight to each patter. Then the algorithm goes through the students' replies and matches the patterns. Thereby it is possible to give a wide range of grades -- based on how much patterns were matched and their weight \cite{Burrows}. 
\item PMatch is suitable only for very short answers -- not longer than one sentence. The system can match short and simple word sequences. It can inform students that words in their replys are either misspelled or unknown to the system and offer spelling correction options ~\cite{Hasanah}, ~\cite{Burrows}.
\end{itemize}
\item the main idea of \textbf{boolean phrase matching} is matching of key-phrases between an example and answers to grade. To be able to deal with synonyms and word forms the systems with boolean matching use thesaurus and lemmatization. Phrases with the same meaning are added to the needed sentence representation as "or" and the required ones -- as "and" ~\cite{Hasanah}. An example of this is shown in Fig.\ref{fig:AndOr}. It is taken from the work by \textbf{Thomas} ~\cite{Thomas}, which describes an example of such an autograding system. Due to "or" the system has a certain flexibility, however, as a correct answer it could only accept a perfect match with one of the phrases from the tree.
\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{img/AndOr}
    \caption{ An example of boolean and/or tree for the answer that must contain the following: "bounds registers", "range memory locations,", "address outside range exception" ~\cite{Thomas} }\label{fig:AndOr}
\end{figure}
\item \textbf{syntactic pattern matching} is a variety of methods using different syntactic structures:
\begin{itemize}
\item \textbf{auto-marking} requires specifically developed grammars those contain the hand-crafted patterns to match. This system is hard to use for people unfamiliar with the rules those were used for grammar construction ~\cite{Hasanah}.
\item In \textbf{Oxford-UCLES} system Pulman and Sukkarieh uses Hidden Markov part-of-speech model in combination with Noun Phrase and Verb Group finite state machine. Their approach excludes templates merging and ambiguity resolution, because they assume that the student replies in their dataset don't contain unnecessary information and ambiguous phrases. IE is combined with such machine learning methods as Naive Bayesian learning, inductive logic programming and decision trees. The best result was achieved with Naive Bayesian learning ~\cite{Pulman}.
\item \textbf{IndusMarker} as well as auto-marking uses specifically constructed grammars for word- and phrase-level matching, but also includes spell checking and correction. The system developers introduced a "Question Answer Markup Language" where the users can input the correct answers ~\cite{Burrows}.
\end{itemize}
\item \textbf{syntactic-semantic pattern matching}:
\begin{itemize}
\item \textbf{eMax} uses both syntactic and semantic analysis. User of the system must provide one or multiple correct answers and highlight important elements of them. It is important that the system provides a confidence level, so a human grader could check the low-confidence answers him/herself ~\cite{Burrows}, ~\cite{Hasanah}.
\item \textbf{FreeText Author} operates on a sentence-level. Every sentence is being semantically analyzed and the system returns a certain feedback that is compared to the feedback from the syntactic-semantic templates those are automatically created based on example answers. User of the system can specify keywords and accepted synonyms. The system has a user-friendly interface and it is easy to use ~\cite{Burrows}, ~\cite{Hasanah}.
\end{itemize}
\item \textbf{semantic word matching} is based on individual terms matching. An example of such a system is \textbf{Auto-Assessor}. To rate answers it uses such NLP tools as bag-of-words, WordNet.NET and SharpNLP. The system can only accept one example answer, but of this example it automatically generates a range of acceptable solutions. It grades based on can only grade one-sentence answers. Auto-Assessor has a user-friendly interface ~\cite{Burrows}, ~\cite{Hasanah}. 
\item \textbf{Lexical Resource Semantics (LRS) representation matching} system -- \textbf{CoSeC-DE} -- was developed by Hahn in 2012. The system uses Corpus of Reading Comprehension Exercises in German (CREG) ~\cite{CREG}. The answers are preprocessed using TreeTagger and dependency parser MaltParser and afterwards the system creates syntax-semantics-interface representations and then builds lexical resource semantics representations. Comparison of example and the answer to grade is performed by alignment afterwards. It is more accurate than bag-of-words model, because it allows to avoid consider a sentence of a higher level and avoid such problems: consider the sentences below: "CoSeC-DE is more advanced than AutoMark", "CoSeC-DE outperforms AutoMark", "AutoMark is more advanced than CoSeC-DE". Bag-of -words would assume that the first and third sentences are the same and the second one is different, whereas the first two are the same and the third one contradicts them. CoSeC-DE evaluates these sentences correctly  ~\cite{Hahn}.
\end{itemize}


\begin{table}[h!]
\centering
\caption{Information extraction systems}
\label{IE}
\begin{tabular}{|l|l|l|l|}
\hline
 System & Year & Approach & Accuracy \\ \hline
 AutoMark & 2002  & parse tree matching & 92.5\% \\ \hline
 WebLAS & 2002  & regular expression matching  & unknown \\ \hline
 PMatch & 2012  & regular expression matching & 90\%  \\ \hline
 Thomas & 2003  & boolean phrase matching & 86\% \\ \hline
 auto-marking & 2003  & syntactic pattern matching &  88\% \\ \hline
 Oxford-UCLES & 2005  & syntactic pattern matching &  87\% \\ \hline
 IndusMarker & 2008  & syntactic pattern matching & 96\% \\ \hline
 eMax & 2009  & syntactic-semantic pattern matching &  unknown \\ \hline
 FreeText Author & 2009  & syntactic-semantic pattern matching &  unknown \\ \hline
 Auto-Assessor & 2011  & semantic word matching  &  unknown \\ \hline
 CoSeC-DE & 2012 & LRS representation matching &  86.3\% \\ \hline
\end{tabular}
\end{table}

From the Gantt chart above and Burrows's review paper one can see that this research direction tends to stay in the past as well as the concept mapping, but Hasanah ~\cite{Hasanah} in 2016 year's paper argues that it is a valid method that can be used in combination with corpus-based methods, which can help to deal with synonyms, in the future.

\subsubsection{Corpus-based methods}

This group of methods is widely used for analyzing of large texts. It has already been considered in AES section, but corpus-based methods can also be useful for ASAG, because they allows a better paraphrasing detection by efficient interpreting of synonyms ~\cite{Burrows}. These are statistical methods those use information from the large amount of text, so short answers should be preprocessed in order for corpus-based methods to be applied. Below the most significant corpus-based methods are considered.\\

\textbf{Atenea} system uses a weighted combination of an extended version of BiLingual Evaluation Understudy (BLEU) metric and LSA ~\cite{ERB}. LSA has already been considered in AES section. BLEU in turn was initially used in machine translation, but later it become used in all fields of machine text-processing including ASAG systems. The key concept of this metric is a modified unified n-gram precision (MUP) measure: all n-grams and number of their occurances are taken into account, n-grams of certain type are summed up and divided by the total number of n-grams. It is calculated in the following way ~\cite{BLEU}:\\

\begin{equation} \label{eq:MUP}
MUP_n = \frac{\sum\limits_{C \in \{AnswerSentences\}} \sum\limits_{n-gram \in C} count_{matches}(n-gram)}{\sum\limits_{C' \in \{AnswerSentences\}} \sum\limits_{n-gram \in C'} count_{matches}(n-gram')}.
\end{equation}

\begin{equation} \label{eq:BP}
BP = \begin{cases}
   1 &\text{if $c>r$}\\
   e^{1-r/c} &\text{if $c \leq r$}
 \end{cases},
\end{equation}

where BP is a brevity penalty, c -- length of the student's answer and r -- effective reference corpus length.

\begin{equation} \label{eq:BLUE}
SIM_{BLUE} = BP \times e^{\sum^N_{n=1}w_n \times log(MUP_n)},
\end{equation}

where N is a maximum size of an n-gram and $w_n$ -- weight of the corresponding n-gram size. For estimation of BLUE similarity usually $log(SIM_{BLUE})$ is used. \\

For convenience of the reader, these and further similarity measures are summarized in a table \ref{sim} in the "State of the art summary" section.\\

An extended version of BLEU -- Evaluating Responses with BLEU (ERB) adds recall accommodating to precision that uses modified brevity penalty (MBP) factor. The ERB similarity is calculated in the following way ~\cite{ERB}:

\begin{equation} \label{eq:ERB}
SIM_{ERB}(answer) = MBP(answer)\times e^{\sum^{N}_{n=0}\frac{log(MUP(n))}{N}},
\end{equation}

where n -- length of n-grams-sequence and N -- highest possible value of n. \\

Another system for corpus-based ASAG is \textbf{Willow} developed on the base of Atenea ~\cite{Willow}. The system is different from the previous ones, because it extensively uses various shallow NLP techniques, such as stemming and lemmatization, disambiguation, and multiword term identification. Basically, Willow is based on ERB with an NLP preprocessing. The architecture of the system is shown in Fig. \ref{fig:WillowArch}. It doesn't include LSA part explicitly, but it is normally being used in combination with LSA: LSA + Willow (NLP (stemming + removal of closed-class words + no disambiguation) + ERB). The combination is calculated in the following way:

\begin{equation} \label{eq:Willow}
SIM_{Willow} = \alpha \times SIM_{PureWillow} + (1 - \alpha \times SIM_{LSA}),
\end{equation}

where $\alpha$ is a parameter that defines the weights of LSA and Willow parts, the best result was reached with $\alpha = 0.489$. WordNet and British National Corpus (BNC) for English and EuroWordNet for Spanish were used for LSA.\\

 \begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{img/WillowArch}
    \caption{ Willow's architecture ~\cite{Willow}, where "CCW" is "closed-class words" and "WSD" -- word sense disambiguation.}\label{fig:WillowArch}
\end{figure}

The system had a user-friendly interface (see Fig. \ref{fig:Willow}) and it worked with English and Spanish.\\

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{img/Willow}
    \caption{ Interface of Willow ~\cite{Willow}. }\label{fig:Willow}
\end{figure}


In \textbf{Short Answer Measurement of Text (SAMText)} author uses modified LSA. He finds the regular LSA unsuitable for ASAG due to the following reasons ~\cite{SAMText}:
\begin{itemize}
\item LSA is ineffective when the answer is shorter than around 200 words, which is often the case in ASAG.
\item Relevant corpus doesn't always exist. LSA trained on general texts can be rather useless for specific areas and it is not always easy to find a large enough corpus for this specific area.
\item Most of the existing LSA-based systems require large pregraded datasets. Therefore, a solution might only work well on some specific dataset and an attempt to extend it to a new area of knowledge would fail. That is one of the reasons why it doesn't make much sense to compare the accuracy of various ASAG systems given in the published papers -- everybody uses their own dataset, which can be overly convenient for their method, and often there is no published code to test the solutions on the similar datasets. 
\item To use LSA one must find an optimal dimensionality.
\end{itemize}
Whereas a regular LSA uses a large corpus to model semantic relatedness, the given LSA modification is based on inverted index data structure, which requires general-corpus and topic-specific keywords to create the corpora. The advantage of the approach is that it allows to use a subject-specific sublanguage  instead of the whole language. Thereby, modified LSA can work properly with short answers and only requires a few example answers ~\cite{Burrows}.\\

\textbf{Mohler} ~\cite{Mohler} focuses on unsupervised ASAG systems and evaluates ten of them on the same dataset obtained from  Data Structures course. It is important, because, as it was mentioned above, it is usually hard to compare the  systems tested on completely different datasets. Moreover, the dataset was published and thereby the other authors later also have the opportunity to use this dataset to evaluate their algorithms ~\cite{Sultan}, which allows to obtain comparable results. The following methods were evaluated: 
\begin{itemize}
\item knowledge-based methods with the following maximum similarity metric:
\begin{equation} \label{eq:maxsim}
maxsim(W,C) = max SIM_x(W,w_i),
\end{equation}
where $C$ is a word class, W -- a word of the class from the example answer, $w_i$ -- a word of the class from the student answer, $SIM_x$ -- one of the following similarity measures:
\begin{itemize}
\item shortest path:
\begin{equation} \label{eq:shortestPath}
SIM_{shortestPath} = \frac{1}{length},
\end{equation}
where $length$ is the shortest path between the concepts calculated based on node-counting. Accuracy: 44.14\%.
\item C-Rater (has already been considered in "Concept/facet mapping" section):
\begin{equation} \label{eq:c-rater}
SIM_{c-rater} = -log \frac{length}{2 \times D},
\end{equation}
where $length$ is calculated similar to the shortest path approach and D is the maximum depth of the taxonomy. Accuracy: 22.31\%.
\item Lesk:
\begin{equation} \label{eq:Lesk}
SIM_{Lesk} = f(two\_concepts\_overlap).
\end{equation}
Accuracy: 36.3\%.
\item Wu and Palmer:
\begin{equation} \label{eq:Wu}
SIM_{Wu} = \frac{2 \times depth(least\_common\_subsumer)}{depth(concept_1) + depth(concept_2)},
\end{equation}
where the concepts are from WordNet taxonomy. Accuracy: 33.66\%.
\item Resnik:
\begin{equation} \label{eq:Resnik}
SIM_{Resnik} = -log P(least\_common\_subsumer),
\end{equation}
where $P(concept)$ is a probability to find an instance of the concept in a given corpus. Accuracy: 25.20\%.
\item Lin:
\begin{equation} \label{eq:Lin}
SIM_{Lin} = \frac{2 \times log P(least\_common\_subsumer))}{log P(concept_1) + log P(concept_2)}.
\end{equation}
Accuracy: 39.16\%.
\item Jiang and Conrath:
\begin{equation} \label{eq:Jiang}
SIM_{Jiang} = - \frac{1}{log P(concept_1) + log P(concept_2) - 2 \times log P(least\_common\_subsumer) }.
\end{equation}
Accuracy: 44.99\%.
\item Hirst and St-Onge:
\begin{equation} \label{eq:Hirst}
SIM_{Hirst} = f(strength\_of\_pair\_of\_synonym\_sets),
\end{equation}
which is based on lexical chains detection in WordNet taxonomy. Accuracy: 19.61\%.
\end{itemize}
The implementation of the methods above is WordNet-based.
\item corpus-based methods :
\begin{itemize}
\item LSA (considered in details in "AES" section) trained on BNC and English Wikipedia (September 2017 version). For implementation of LSA InfoMap package (\url{http://infomap-nlp.sourceforge.net/}) is used. Accuracy: 67.35\% -- on reduced computer science Wikipedia corpus with adding new good students' answers to the examples list.
\item Explicit Semantic Analysis (ESA or ESA Wikipedia) -- concept-vector features approach trained on Wikipedia (September 2017 version) corpus, each article is a concept vector. This algorithm is implemented by the authors themselves based on work by Gabrilovich and Markovitch ~\cite{Gabrilovich}. Accuracy: 64.98\% -- on full Wikipedia corpus with adding new good students' answers to the examples list.
\end{itemize}
It was shown that the accuracy of LSA strongly depends on the size of training corpus and on how close the corpus topic is to the topic of exam, i.e. the better results for the Data Structures test was obtained using domain-specific corpus that contained Wikipedia articles on computer science. However, for ESA the size is more important and it performs worse on a smaller domain-specific corpus. Also, in addition to the example correct answers the system added the best student answers to the list of examples, which helped to improve the quality of grading significantly. 
\end{itemize}

It is interesting that the result of LSA, which was said to be unsuitable for ASAG in Bukai's work ~\cite{SAMText} shows the best results in Mohler's work ~\cite{Mohler}. However, the best result is 67.35\%, therefore it is still questionable, if LSA сan be used for ASAG. \\

As it was shown by Mohler ~\cite{Mohler}, LSA performs better when trained on domain-specific corpus. Therefore, \textbf{Klein} ~\cite{Klein} trains LSA on even more specific corpus that consists only of partially manually-graded student's answers. To determine the distance between the answers, a linear combination of cosin and Euclidean similarities was chosen:

\begin{equation} \label{eq:cos}
SIM_{cos} = \frac{\pmb{answer_1} \times \pmb{answer_2}}{||\pmb{answer_1}|| \times ||\pmb{answer_2}||},
\end{equation}
\begin{equation} \label{eq:Euclidean}
SIM_{Euclidean} = 1 - \sqrt{\sum^k_{i=1}(answer_{1_i} - answer_{2_i})^2},
\end{equation}
\begin{equation} \label{eq:combinedCE}
\begin{array}{l}
SIM_{combined} = \alpha  \times SIM_{Euclidean}(\pmb{answer_1}, \pmb{answer_2}) + \\ + (1-\alpha) \times SIM_{cos}(\pmb{answer_1}, \pmb{answer_2}),
\end{array}
\end{equation}\\
 where $\alpha$ is a parameter between 0 and 1 that allows to assign a weight to the distances. However, it was shown that the best result is reached when $\alpha = 1$, i.e. when only cosine similarity is used. \\
  
The system would find the graded answer, which is the most similar to the student's answer, and gives it the corresponding grade. It is important that in case if there is no graded assignment that is similar enough, the student's answer is marked as "unassigned". LSA here is used in combination with clustering. Three clustering methods to choose the assignments to grade manually were used:
\begin{itemize}
\item random choice of assignments to be graded manually
\item k-means clustering to divide the students' answers into semantically-similar clusters: one or two answers from each cluster should be graded manually
\item modified similarity-based Min-Max algorithm: first answer to grade manually is chosen randomly and the new ones are chosen in such a way that the answer should be the least similar to the closest answer in already built manually-graded set:\\ $index = argmin_{index}(max_j (SIM_{combined} (\pmb{answerToAdd_{index}}, \pmb{gradedAnswer_j})))$
\end{itemize}
The Min-Max algorithm was shown to be the most efficient and k-means was on the second place: the total amount of answers was 49, Min-Max algorithm allowed to reduce the number of manually graded answers to 35, k-means -- to 40 and random choice -- between 41 and 45. The accuracy of the grading is fairly high -- between 80\% and 95\%. However, to really reduce the teacher's workload, it would require a lot of pregraded works from previous exams those must be similar to the current one, otherwise it only allows to reduce the number of works to grade by around 20\%-30\%, which doesn't worth the effort. \\

The work of \textbf{Noorbehbahani} ~\cite{Noorbehbahani} deserves a special attention, for it contains a good description of various metrics:

\begin{itemize}
\item n-gram co-occurrence --  number of similar n-grams:\\
$SIM_{n-gram} = \frac{N_{sim}}{N_{total}},$\\
where $N_{sim}$ -- number of matching n-grams between the student's and example answers and $N_{total}$ -- total number of n-grams in student's answer.
\item BLUE -- has already been considered above in this section -- in the description of Atenea system, see equation \ref{eq:BLUE}. In addition to that
Noorbehbahani mentions the following drawbacks of the system ~\cite{Noorbehbahani}:
\begin{itemize}
\item BLUE only recognizes exactly-matching words, which means that it discards even misspelled words, and will certainly ignore synonyms;
\item all words have the same weights, which means that the essential for the meaning of the answer special terms would have as much meaning as articles or transition words;
\item the system compares the student answer with all reference answers and counts matches from different references;
\item brevity penalty punishes all answers those are longer than the reference ones, which doesn't always makes sense.
\end{itemize}
\item M-BLUE -- the system proposed by Noorbehbahani. It is based on the same idea, but includes the following modifications:
\begin{itemize}
\item spelling correction;
\item acceptance of synonym-n-grams, understanding of paraphrasing: the system generates synonym example answers based on the given reference answers in such a way that it is as close as it is possible to the student answers, but still keeps the meaning of the reference answers;
\item weighting of the n-grams by their importance, i.e. special terms should have higher weights, whereas an n-gram like "and the" can have a nearly-zero weight;
\item the system compares student's answers with all the example answers, but the final similarity measure is calculated only with the most similar one.
\item brevity penalty is calculated based on weights, because the length of the answer doesn't help to estimate the correctness;
\item the final M-BLUE similarity measure is calculated in the following way:
\begin{equation} \label{eq:M-BLUE}
SIM_{M-BLUE} = \lambda \times BP_r \times SIM_{M-BLUE} + (1 - \lambda) \times S_0,
\end{equation}
where $BP_r$ is a modified weighted brevity penalty, $SIM_{BLUE_ra}$ -- weighted similarity measure between a student answer and one reference answer, $S_0$ --  measure for calculating the common-words order similarity and $\lambda$ is a similarity components weight parameter, which is taken in range between 0.5 and 1. In Noorbehbahani's work $\lambda = 0.85$ was chosen experimentally. The whole derivation of the equation \ref{eq:M-BLUE} is intentionally omitted, because the meaning of the equation members is clear from the list items above and the formulas for them can be found in ~\cite{Noorbehbahani}.
\end{itemize}
\item ERB -- has already been considered above in this section -- in the description of Atenea system, see equation \ref{eq:ERB}.
\item Recall-oriented understudy for gisting evaluation (ROUGE) was initially developed for evaluation of quality of the summaries. There are the following implementation of ROUGE:
\begin{itemize}
\item ROUGE-N -- estimates n-gram with $n>1$ co-occurrences, similar to BLEU.
\item ROUGE-L -- finds and calculates the length of longest common subsequence of two answers.
\item ROUGE-W -- finds similar subsequences and takes them into account according to weights.
\item ROUGE-S -- uses skip-bigram.
\item ROUGE-SU -- uses the same idea that ROUGE-S, but also takes into account unigrams.
\end{itemize}
\item Combination of LSA and n-grams -- answers are broken into sentences, each of them is a vector for LSA and closeness of the answers is evaluated as a cosin distance. Separately n-gram n-gram co-occurrence is calculated. After this, two similarities are combined:\\
\begin{equation} \label{eq:LSA+n-grams}
SIM_{LSA+n-grams} = \frac{SIM_{LSA} + SIM_{n-gram}}{2}.
\end{equation}
The method can be modified by a more complex -- weighted -- calculating of the similarity.
\end{itemize}

The dataset used in this paper was obtained from a computer science e-learning course. It contained 45 questions, 300 example answers and 237 student's answers.\\

There were several important papers on corpus-based methods published 2012. The first one by \textbf{Li} ~\cite{Li} concentrates on various similarity metrics, which allows us to extend the list of the metrics given by Mohler and Noorbehbahani. The following similarity metrics are considered:

\begin{itemize}
\item cosine similarity -- has already been considered above in this section -- in the description of Mohler's system. \\
Accuracy: 75.3\%.
\item Jaccard similarity -- percent of intersection of two sets (words of answers):
\begin{equation} \label{eq:Jaccard}
SIM_{Jaccard} = \frac{answer_1 \cap answer_2}{answer_1 \cup answer_2}.
\end{equation}
Accuracy: 73.9\%.
\item S{\o}rensen-Dice coefficient -- a number of overlap words in two answers:
\begin{equation} \label{eq:Dice}
SIM_{Dice} = \frac{2 \times (answer_1 \cap answer_2)}{|answer_1| + |answer_2|}.
\end{equation}
Accuracy: 76.6\%.
\item Manhattan similarity -- introduced by Li for ASAG. Each answer is represented by a vector, e.g. $ \pmb{answer_1} = (answer_{1_1}, answer_{1_2},..., answer_{1_n})$, where n is a length of the answer in words. The similarity is calculated between the student's answer and multiple example answers in the following way:
\begin{equation} \label{eq:Manhattan}
SIM_{Manhattan} = 1 - \sum^n_{k=1}|answer_{1_k} - answer_{2_k}| / m,
\end{equation}
where m is the total number of words in two answers.\\
As a reference answer the most similar example answer is chosen, i.e. similarity between this and student answer would be taken into account for grading.\\
Accuracy: 76.2\%.
\item Keywords coverage rate -- basically it is a count of present and absent keywords or their synonyms. For this Levenshtein distance ("minimum number of operations needed to transform one string into the other" ~\cite{Gomaa}) is used, because it allows to evaluate substitutions and deletions in strings. \\

\begin{equation} \label{eq:Levenshtein}
SIM_{Levenshtein} = \frac{max(length(currentWord,keyword)) - minLev}{length(keyword)},
\end{equation}
where $minLev$ is the distance between the current keyword the most similar word in a student's answer. If it is larger or equal to the minimal length of the two words, $SIM_{Levenshtein}$ nullifies. 
\begin{equation} \label{eq:keywordCover}
keywordCoverage = \sum_{i} SIM_{Levenshtein} / numberOfKeywords,
\end{equation}
Accuracy: 64.6\%.
\end{itemize}

The similarity measures were tested separately and then combined with different weights those were determined by linear regression.\\

A distinctive feature of this paper is that they evaluate non written, but spoken answers, which makes preprocessing harder, and requires speech-recognition tools instead of, for example, spelling correction. However, the rest of the work is similar to the regular written ASAG. The author claims that the accuracy of the method is 77.8\% and it is very impressive, eepesially taking into account that the correlation between the human raters was 80.5\%. However, the used dataset was rather simple. Here are some examples of the answers: "Hello, Xiao Li! This is Bill speaking. What's up?", "Good idea! How much is a ticket?", "All rightˊSee you then! Bye!" ~\cite{Li}. It would be a valid assumption that the results on more complex dataset, e.g. Mohler's one, would be much lower, because even the keyword spotting alone has given 64.6\% correlation with a human grader. Though the results of this work might be not very impressive, it is a good article to move from corpus-based methods to machine learning, because the approach of using various distances as different features will be helpful for machine learning.\\

Another paper published in 2012 by \textbf{Gomaa} ~\cite{Gomaa} also uses a combination of several types of methods -- on string-based and corpus-based. String-based methods are represented by the following:
\begin{itemize}
\item Character-based distance measures
\begin{itemize}
\item Levenshtein distance -- described in the work considered before.
\item Jaro-Winkler distance -- calculates the number of similar characters in two answers taking into account their order. Similarity is calculated in the following way ~\cite{Jaro}
\begin{equation} \label{eq:Jaro}
SIM_{Jaro} = \begin{cases}
   0 &\text{0 if $m = 0$}\\
   \text{$\frac{1}{3}(\frac{m}{answer_1} + \frac{m}{answer_2} + \frac{m - t}{m})$}&\text{otherwise},
 \end{cases}
\end{equation}
where m is the number of matching characters and t -- half of the total transpositions number.
\item Needleman-Wunsch algorithm -- dynamical-programming algorythm initially used for protein or nucleotide sequences similarity calculations. It searches for the best alignment over the whole two answers.
\item Smith-Waterman algorithm -- also from dynamic programming, looks for similarities over conserved domain of two answers. 
\item n-gram co-occurrence -- considered above in the description of Noorbehbahani's work.
\end{itemize}
\item Term-based distance measures
\begin{itemize}
\item Manhattan distance -- considered above in the description of Li's work.
\item cosine similarity -- considered above in the description of Mohler's system.
\item S{\o}rensen-Dice distance -- considered above in the description of Li's work.
\item Euclidean distance -- considered above in the description of Mohler's system.
\item Jaccard similarity -- considered above in the description of Mohler's system.
\item Matching Coefficient -- number of non-zero values over the term axes those are similar for two answer-vectors.
\item Overlap coefficient -- modified S{\o}rensen-Dice distance: if only a subset of one answer is similar to the whole other answer, these answers are 100\% match.
\end{itemize}
\end{itemize}

Corpus-based similarity methods are represented by the following:

\begin{itemize}
\item LSA -- considered above in AES section.
\item ESA -- considered above in the description of Mohler's system.
\item Pointwise Mutual Information -- uses  AltaVista's Advanced Search query syntax to calculate the matching between word pairs by calculating the probability of co-occurrence of the words based on web pages corpus.
\item extracting DIstributionally related words using CO-occurrences (DISCO) -- Java application for calculating similarity between words or short texts, finding the most similar words in texts, e.g. "smart" and "clever", and finding the related words, e.g. "beer" $\rightarrow$ "K\"olsch", alcohol, foam, barrel. It requires a database of word similarities (word space) precalculated on a large text corpora by "DISCO Builder". It is similar to word2vec --  log-bilinear prediction-based bag-of-words and skip-n-gram algorithms for vectorizing words developed by Google  ~\cite{word2vec} -- and Global Vectors for Word Representation (GloVe) -- log-bilinear regression unsupervised algorithm for word vectorization ~\cite{GloVe}. Architecture of the system is shown in Fig. \ref{fig:DISCO}. DISCO Builder is trained on Wikipedia and BNC. 
\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{img/DISCO}
    \caption{ Architecture of DISCO ~\cite{DISCO}. }\label{fig:DISCO}
\end{figure}
\end{itemize}

ASAG is performed in three stages:
\begin{itemize}
\item answer similarities are calculated by all string-based methods above.
\item DISCO similarities are calculated, at the same step lammatization and stop-words removal are made.
\item combination of first two steps and deriving the grade based on maximum and average overall similarities
\end{itemize}
To evaluate the efficiency of the introduced methods, Mohle'r dataset is used. The best human-computer correlation among string-based methods was achieved by n-gram co-occurrence -- 39.8\%. The best DISCO result was shown after training on Wikipedia corpus -- 47.5\%. The three methods mentioned above were combined and the total result reached 50.4\%. This work shows not very high results in comparison to many other papers, however, there are several important points:
\begin{itemize}
\item it uses Mohler's dataset, which  makes the result comparable. Moreover, the obtained result is not that low if we compare it to the other algorithms trained on the same data.
\item The algorithm was trained on whole Wikipedia corpus. It is very likely that the result would be better if only computer science Wikipedia would have been used, as it helped to improve LSA's performance in Mohler's work.
\item The work also shows the importance of combination of various metrics and methods and thereby leads to using them as features in machine learning.
\end{itemize}
 
Table \ref{Corpus} shows the comparison of various corpus-based methods those are in part of the cases combined with various similarity metrics. A dataset column was not added intentionally, because most of the works use their own datasets. The works using Mohler's dataset marked as "(MD)".
 
\begin{table}[h!]
\centering
\caption{Corpus-based systems and metrics}
\label{Corpus}
\begin{tabular}{|l|l|l|l|}
\hline
 System & Year & Approach & Accuracy \\ \hline
 Atenea & 2004 & ERB + LSA & 50\% \\ \hline
 Willow & 2006 & ERB + LSA + NLP & 56\% \\ \hline
 SAMText & 2006 & modified LSA & 69\%-72\% \\ \hline
 Mohler & 2009 & LSA & 67.35\% (MD) \\ \hline
 Klein & 2011 & LSA + clustering &  80\%-95\% \\ \hline
 Noorbehbahani & 2011 & M-BLEU & 85\% \\ \hline
 Gomaa & 2012 & bag of words & 50.4\% (MD) \\ \hline
 Li & 2012 & Approach & 77.8\% \\ \hline
\end{tabular}
\end{table}
 
\subsubsection{Machine learning}

Machine learning (ML) requires a preprocessing that would allow to retrieve features for further reduction of the ASAG task to regression or classification problem. Previous section of the work focused on corpus-based methods and similarity metrics. First works were mostly using one or two metrics combined with certain weights. However, the latest works started using a combination of several techniques with automatically chosen wights and it was shown that this approach gives better results. This led to an important idea of combination of several techniques as features for further machine learning. And moreover, to an idea of combination of several machine learning techniques in the later machine learning works. \\

The oldest ML system -- \textbf{e-Examiner} ~\cite{e-examiner} focuses various ROUGE metrics (considered in details in "Corpus-based methods and metrics" section) in combination with cosine distance and such NLP techniques as tocenization, morphological analysis and stopword finding and removal. The architecture of the system is shown in Fig. \ref{fig:e-examiner}. It is a  Web-based stand-alone service written in Java that uses MySQL to store the answers.\\


\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{img/e-examiner}
    \caption{ Architecture of e-Examiner ~\cite{e-examiner}. }\label{fig:e-examiner}
\end{figure}

The system has a user-friendly interface, where user can choose which of the ROUGE techniques should be applied -- see Fig. \ref{fig:e-examiner_Interface}.\\

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{img/e-examiner_Interface}
    \caption{ Interface of e-Examiner ~\cite{e-examiner}. }\label{fig:e-examiner_Interface}
\end{figure}

To evaluate the performance of the system datasets from The Institute for Information Systems and New Media (IICM) and Faculty of Engineering, Al-Quds University, Jerusalem were chosen. IICM consists of 5 computer science questions, 5 example answers and 23 sets of students' answers. From the second dataset were taken 3 questions, 3 example answer and also 23 sets of student's answers. The best result -- 81\% -- was achieved using a combination of all ROUGE techniques and cosine similarity.\\

Content Assessment Module (\textbf{CAM}) ~\cite{CAM} is a system for grading of assignments on  English as a Second Language (ESL) test. A dataset for evaluation of the system is taken from this test. It includes 75 questions 566 students' answers, having an example answer is not required. In this method the answers are divided into a graded and test sets and the answers from the test set are compared to the ones from the graded set using a number of NLP techniques: tokenization, lemmatization, spelling correction and noun phrase chunking -- for preprocessing, and similarity metrics -- for preprocessed text similarity calculation. For these tasks the following tools were used:
\begin{itemize}
\item MontyLingua -- Java/Python NLP toolkit (\url{http://alumni.media.mit.edu/~hugo/montylingua/doc/MontyLingua.html}),
\item PC-KIMMO --  morphological analysis two-level processor  (\url{http://www.ai.mit.edu/courses/6.863/doc/pckimmo.html}),
\item Spell Checker Oriented Word Lists (SCOWL) -- spelling correction for English (\url{https://github.com/en-wl/wordlist}),
\item TreeTagger -- part-of-speech annotation tool for many languages (\url{http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/}),
\item CASS -- a tool for chunking (\url{http://spraakdata.gu.se/svedk/cass_swe.html}),
\item WordNet -- considered in "Information extraction methods" section,
\item PMI-IR -- considered in "Corpus-based methods and metric" section,
\item Stanford parser -- a statistical tool for structure of sentences parsing (\url(https://nlp.stanford.edu/software/lex-parser.shtml)).
\end{itemize}


\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{img/CAM}
    \caption{ Architecture of CAM ~\cite{CAM}. }\label{fig:CAM}
\end{figure}


 The following features were used ~\cite{CAM}:
\begin{itemize}
\item Keyword overlap -- percent of graded-test aligned keywords 
\item Graded and test overlaps -- percents of aligned graded and test tokens 
\item Graded and test chunk -- percents of aligned graded and test chunks
\item Graded and test trigrams -- percent of aligned graded and test trigrams
\item Unigram match -- percent of identical graded and test unigrams
\item Similarity match -- percent of similar tokens
\item Type match -- percent of type-resolved token alignments
\item Lemma match -- percent of lemma-resolved token alignments
\item Synonym match -- percent of synonym-resolved token alignments
\item Variety of match -- number of token-level alignments types
\end{itemize}

These features are used with Tilburg Memory-Based Learner (TiMBL) classifier and thereby reach 88\% of human-computer correlation. TiMBL is based on k-nearest neighbor (KNN) classifier modified in such a way that the nearest neighbors are searched in a decision-tree structure ~\cite{TiMBL}. It is open-source and can be downloaded on the following website: \url{https://languagemachines.github.io/timbl/}. The last commit in TiMBL's github repository (\url{https://github.com/LanguageMachines/timbl}) was on 7th of February 2018, which shows that it is still relevant. \\

Comparing Meaning in Context (\textbf{CoMiC-EN}) ~\cite{CoMiC-EN} and \textbf{CoMiC-DE} ~\cite{CoMiC-DE} are systems for grading reading comprehension assignments in English and German correspondingly. The work is strongly based to CAM approach. It uses the same dataset (for CoMiC-EN) and ML method -- TiMBL. However, there are the following differences:

\begin{itemize}
\item whereas there is only strict unigram-unigram and chunk-chunk comparison in CAM, CoMiC can also compare unigrams with chunks. It is important, because a several words often have a similar meaning to the one word, e.g. "green-eyed monster"-"jealousy".
\item whereas CAM just removes punctuation and words already mentioned in a question from the answers, CoMiC keeps it to save the coherence of the sentence.
\item an updated architecture.  As a basis for its architecture CoMiC employs  Unstructured Information Management Architecture (UIMA), which is an efficient architecture for working with large amounts of unstructured information -- "whose intended meaning is only loosely implied by its form" ~\cite{UIMA}, which is natural language texts, pictures, videos, etc. Structured information would be the one that has a direct relation between the information and form, e.g. a database. UIMA-based architecture of CoMiC is shown in Fig. \ref{fig:UIMA}.

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{img/UIMA}
    \caption{ Architecture of CoMiC ~\cite{CoMiC-EN}. }\label{fig:UIMA}
\end{figure}

\item CoMiC uses different NLP tools for their extraction:
\begin{itemize}
\item Apache OpenNLP toolkit (\url{https://opennlp.apache.org/}) instead of MontyLingua and CASS,
\item Morpha -- Java tool for morphological analysis (\url{https://github.com/knowitall/morpha}) instead of PC-KIMMO,
\item MaltParser -- a data-driven dependency parsing (\url{http://www.maltparser.org/}) instead of Stanford parser.
\end{itemize}
The rest of the tools (SCOWL, TreeTagger, WordNet and PMI-IR) remained the same.
\item CoMiC uses the following features: "cosine distance, dot product, weighted overlap, Levenshtein distance, Euclidean distance, modified value difference, Jeffrey divergence and numeric overlap" ~\cite{CoMiC-EN}. All of the have already been described above.
\item CoMiC-EN is not only capable to estimate if the answer was correct or not, bit also provide a detailed information, i.e. which concepts were missing and which were added.
\end{itemize} 

CoMiC shows an insignificantly better result than CAM -- 88.4\% vs 88\%. Nevertheless, the main idea of the paper was to introduce an UIMA architecture and new NLP tools for ASAG. It is also important that CoMiC-EN corpus is available for other researchers, which allows them to compare their results with the ones in the paper. It is not suitable for the current work though, because we focus on computer and natural science.\\

CoMiC-DE is conceptually identical to CoMiC-EN, but focuses on German languag and uses Corpus of Reading comprehension Exercises
in German (CREG). It is the first work that considers automated grading of reading comprehension in German. One can suggest that English and German languages might require slightly different NLP-prepsocessing, however, in this paper the same tools are used and results of English and German ASAG were identical.\\

In \textbf{Zbontar}'s ~\cite{Zbontar} paper a stacking method is used. The main idea of the stacking is combining not only of several metrics, but also of several machine learning methods. The following preprocessing was used: lowercasing, removal of all punctuation, spelling correction, breaking text into 4- and 6-character-grams and creating bag-of-words model based on the n-grams. It is important that character n-grams were used, for usually sentences are broken into word-n-grams. Word-n-grams were already considered before. Character n-grams, e.g. with n = 4, would break a sentence in the following way: "finnegans wake" = \{"finn", "inne", "nneg", "nega", "egan", "gans", "ans ", "ns w", "s wa", " wak", "wake"\}. The following machine learning methods were used: Ridge Regression (RR), SVM -- with lineal and RBF kernels, Gradient Boosting Machine (GBM), Random Forests (RF) and KNN. Different combination of methods were used with different preprocessing:
\begin{itemize}
\item RR, GBM and RF -- with 4-grams
\item RR, SVM-RBF, GMB -- with spelling correction + 4-grams
\item RR, SVM-RBF -- with 6-grams
\item RR, SVM-RBF, KNN -- with spelling correction + 6-grams
\item SVM-RBF -linear, KNN -- with bag-of-word model based on 4-grams + latent semantic indexing with 200 components // TODO: what it is
\item SVM-RBF, GBR, KNN -- with bag-of-word model based on 4-grams + latent semantic indexing with 500 components
\end{itemize}

For evaluation of the method a Kaggle dataset ~\cite{kaggleSas} was used. As Mohler's dataset, this one also contains grades of two human graders. To avoid ambiguity, author just discards the answers graded differently. For the rest of the answers only the grade given by the first human was considered for the learning.

\begin{table}[h!]
\centering
\caption{Machine learning systems}
\label{ML}
\begin{tabular}{|l|l|l|l|}
\hline
System & Year & Approach & Accuracy \\ \hline
e-Examiner & 2007 & \begin{tabular}[x]{@{}c@{}} ROUGE (all) + cosine similarity + \\linear regression \end{tabular} & 81\% \\ \hline
CAM & 2008 &  \begin{tabular}[x]{@{}c@{}} keyword, token, chunk and\\ triple overlaps+\\+ TiMBL-based classifier \end{tabular}  & 88\% \\ \hline
CoMiC-EN/-DE & 2011 &  \begin{tabular}[l]{@{}c@{}} keyword, token, chunk and\\ triple overlaps+\\+ TiMBL-based classifier \end{tabular}  & 88.4\% \\ \hline
Hou & 2011 &  \begin{tabular}[l]{@{}c@{}}   \end{tabular}  &  \% \\ \hline
Zbontar & 2012 & \begin{tabular}[l]{@{}c@{}} bag-of-words of\\character-n-grams\\RR, SVM-RBF,\\GMB, KNN, RF \end{tabular}  &  73.892\% \\ \hline
Horbach & 2013 & \begin{tabular}[l]{@{}c@{}}   \end{tabular}  &  \% \\ \hline
Madnani & 2013 & \begin{tabular}[l]{@{}c@{}}   \end{tabular}  &  \% \\ \hline
SemEval & 2013 &  \begin{tabular}[x]{@{}c@{}} \end{tabular}  & \% \\ \hline
Sultan & 2016 & \begin{tabular}[l]{@{}c@{}}   \end{tabular}  &  \% \\ \hline
Zhang & 2016 & \begin{tabular}[l]{@{}c@{}}   \end{tabular}  &  \% \\ \hline
Pribadi & 2017 & \begin{tabular}[l]{@{}c@{}}   \end{tabular}  &  \% \\ \hline
\end{tabular}
\end{table}


 

\begin{table}[h!]
\centering
\caption{Machine learning systems}
\label{ML}
\begin{tabular}{|l|l|l|l|}
\hline
 System & Year & Features &  Learning algorithm\\ \hline
 CAM & 2008 & \begin{tabular}[x]{@{}c@{}} teacher-student answers \\ overlap \end{tabular} & k-nearest neighbor classifier \\ \hline
 Nielsen '08 & 2008 & \begin{tabular}[l]{@{}c@{}} lexical \\ and syntactic features \end{tabular} & C4.5 decision tree \\ \hline
 CoMiC-EN & 2011 & \begin{tabular}[l]{@{}c@{}} keyword, token, chunk and\\ triple overlaps \end{tabular}  & TiMBL-based classifier\\ \hline
 CoMiC-DE & 2011 & \begin{tabular}[l]{@{}c@{}} keyword, token, chunk and\\ triple overlaps \end{tabular}  & TiMBL-based classifier \\ \hline
 Hou '11 & 2011 & \begin{tabular}[l]{@{}c@{}} POS tags, term frequency,\\ tf.idf, and entropy \end{tabular} & SVM  \\ \hline
 Horbach '13 & 2013 & \begin{tabular}[l]{@{}c@{}} n-gram features + used in \\ CoMiC-EN  \end{tabular}  & k-nearest neighbors \\ \hline
 Madnani '13 & 2013 & \begin{tabular}[l]{@{}c@{}} BLEU, ROUGE, \\number of sentences \end{tabular}  & logistic regression classifier \\ \hline
 Tandella '12 & 2012 & four- and six-grams & \begin{tabular}[l]{@{}c@{}}Stacking of the following:\\ random forests, \\ k-nearest neighbor \\ classifier, SVM, \\ gradient boosting machine, \\ ridge regression)  \end{tabular} \\ \hline
\begin{tabular}[l]{@{}c@{}} Deep Learning \\Student Modeling \\Clustering \end{tabular}  & 2016 & \begin{tabular}[l]{@{}c@{}} text similarity between \\ student and example answer\end{tabular} &  \begin{tabular}[l]{@{}c@{}}  Deep Belief Networks\\ with restricted \\ Boltzmann machine \end{tabular} \\ \hline
\end{tabular}
\end{table}

Deep learning methods were already widely applied to AES, but so far there is only one paper that applies a deep learning approach to ASAG ~\cite{Zhang}. The author proposes usage of deep belief networks (DBN) that uses restricted Boltzmann machine and compares it with classic classifiers such as naive Bayes, logistic regression, decision tree, artificial neutral network and SVM. It was shown that DBN outperforms the other classifiers, except for SVM and SVM is better only in sensitivity. However, the paper focuses mostly on various text similarity measurements, which alone, as mentioned in ~\cite{Sultan}, is not an adequate measure of answer accuracy. \\


\subsubsection{State of the art summary}

"Natural language processing development" section provides a brief overview of a language structure and the general trends of NLP development (Fig. \ref{fig:NLPLevels}). Understanding of linguistic base of NLP helps to see how it is possible to preprocess text in order to improve the process of grading, e.g. by spelling correction, lemmatization and disambiguation. Moreover, it shows the importance of usage of sublanguage that can reduce the amount of words from thousands to several hundreds. Considering various fields of NLP is also important, because ASAG is not a separate scientific discipline -- most or all of the ASAG techniques has already been used in other NLP-fields, for example, BLUE -- for translation and ROUGE -- for evaluation of summaries. It is always important to look for the new methods in various adjacent fields.\\

AES is the most adjacent to ASAG area. The very first AES and ASAG methods were on the opposite sides of automated grading methods: for ASAG it was keyword-spotting, a very specific approach, and for AES it was evaluation of the most general features -- the length of words and sentences. However, in later works one can see how these two areas are moving towards each other and towards understanding of real meaning and style of the text. ASAG and AES share some common methods, like LSA and various metrics, but all of the methods from AES are interesting for ASAG. For example, it is much less than AES vulnerable to gibberish on the word level. \\

AES research started earlier than ASAG and that is why papers on this topic contain some useful general information. The first important idea is the comparison of human graders and computers. The correlation between the human rates was lower -- around 76\% -- than even the very first AES system. ASAG questions are more straightforward, which usually rises the correlation between humans to 80\%-85\%, but it is still far from perfect. That is why 80\%-90\% result for an ASAG system is already satisfying. Moreover, it is not possible to reach 100\% correlation with a human rater, because the human raters simply give different grades.\\

Another important moment is the reason why the first AES system achieved good results. It was trained on rather convenient dataset. The system was vulnerable to any sort of gibberish, but was only trained and tested on a meaningful dataset. It shows that it is always important to test the method on different datasets and, to compare to other works, try to use the same datasets those were used in those works. Moreover, it is essential to keep in mind that certain types of gibberish, namely "word salad" and "bad faith", can become a problem not only for an AES system, but also for ASAG. The importance of datasets one can see also in further ASAG papers, when in the paper that presents the system it achieves 87\% to 94\% ~\cite{c-rater} and, when tested on another dataset by other author, it reaches only 22\% ~\cite{Mohler}. That is why all the accuracies provided in the tables above or below don't provide enough clarity on how good the methods are. However, without testing the each system manually on different kinds of data, it is the only way to compare them at least approximately. The performance of the most significant ASAG systems is shown in Table *ref*.

-- best methods table

Current work focuses on machine learning methods for ASAG and that is why concept or facet mapping and information extraction are considered mostly to complete the picture. However, the corpus-based section was considered in details, because many machine learning approaches often use similarity metrics those were mentioned in the "Corpus-based methods" section. In table \ref{sim} the similarity metrics are summarized. The accuracy column is taken from Noorbehbahani's ~\cite{Noorbehbahani} (marked as "N"), Mohler's ~\cite{Mohler} ("M"), Li's ("L") and Gomaa's ("G") works. It is important not to forget that the accuracy was obtained on different datasets, and therefore the comparison is not entirely valid. Also, Noorbehbahani doesn't mention, which of the ROUGE methods was used, and it is confusing, because in other papers *ref* it performed significantly better. //TODO: add formulas and  Mohler's metrics. Add ROUGE ref\\

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.7\textwidth]{img/ruleVSstat}
    \caption{  trade-offs between rule-based and statistical methods~\cite{Burrows}. }\label{fig:rVSs}
\end{figure}

Dealing with two human graders -- consider only one -- Zbontar. Discard the discord.

Mostly used languages: Java. Python.\\

Add a list of actually used systems -- the ones with interface and stuff\\

Add a tool table -- word2vec, glove, disco, montilingua, etc.\\

English/German -- the same NLP tools used in CoMiC-EN and -DE. \\

Add list of corpora (wiki \url{https://en.wikipedia.org/wiki/List_of_text_corpora}):
Google Books Ngram Corpus[1][2]
American National Corpus
Bank of English
British National Corpus
Corpus Juris Secundum
Corpus of Contemporary American English (COCA) 425 million words, 1990–2011. Freely searchable online.
Brown Corpus, forming part of the "Brown Family" of corpora, together with LOB, Frown and F-LOB.
International Corpus of English
Oxford English Corpus
Scottish Corpus of Texts \& Speech
Corpus Resource Database (CoRD), more than 80 English language corpora.[3]
RE3D (Relationship and Entity Extraction Evaluation Dataset)

\begin{table}[h!]
\centering
\caption{Similarity metrics}
\label{sim}
\begin{tabular}{|l|l|l|}
\hline
 System & Formula & Accuracy \\ \hline
 ROUGE (N)& 2009 & 22\%  \\ \hline
 BLEU (N)& 2006 & 36\% \\ \hline
 ERB (N)& 2009 & 41\%  \\ \hline
 n-gram co-occurrence (N)& 2006 & 48\% \\ \hline
 LSA & n/a & 51\% (N) or 67.35\% (M) \\ \hline
 LSA \& n-grams (N)&  & 59\%  \\ \hline
 M-BLEU (N)& 2011 & 85\% \\ \hline
\end{tabular}
\end{table}